\documentclass[a4paper,chapter,microtype,fleqn,oneside]{oblivoir}
\usepackage[hangul]{kotex}
\usepackage[inner=1.1in,outer=1.1in]{geometry}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[style=authoryear-comp,sortcites=false,uniquename=false,maxnames=4,minbibnames=99,maxbibnames=99,dashed=false,backref=true,backrefstyle=none]{biblatex}
\usepackage{tcolorbox}
\usepackage{csquotes}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{etoolbox}
\usepackage{caption}
\usepackage{multido}
\usepackage{enumitem}

\include{preamble}

\include{bib_style}

\begin{document}

\tableofcontents

\include{ch_introduction}

\part{Tabular Solution Methods}\label{pt:tabular-solution-methods}

\chapter{Multi-armed Bandits}\label{ch:multi-armed-bandits}

\chapter{Finite Markov Decision Processes}\label{ch:finite-markov-decision-processes}

\chapter{Dynamic Programming}\label{ch:dynamic-programming}
\section{Policy Evaluation (Prediction)}
\section{Policy Improvement}\label{sec:policy-improvement}
\section{Policy Iteration}
\section{Value Iteration}
\section{Asynchronous Dynamic Programming}
\section{Generalized Policy Iteration}\label{sec:generalized-policy-iteration}

\include{ch_monte_carlo_methods}

\chapter{Temporal-Difference Learning}\label{ch:temporal-difference-learning}

\chapter{$n$-step Bootstrapping}\label{ch:n-step-bootstrapping}

\chapter{Planning and Learning with Tabular Methods}\label{ch:planning-and-learning-with-tabular-methods}

\part{Approximate Solution Methods}\label{ch:approximate-solution-methods}

\chapter{On-policy Prediction with Approximation}\label{ch:on-policy-prediction-with-approximation}
\section{Value-function Approximation}
\section{The Prediction Objective (\textoverline{VE})}
\section{Stochastic-gradient and Semi-gradient Methods}
\section{Linear Methods}
\section{Feature Construction for Linear Methods}
\section{Nonlinear Function Approximation: Artificial Neural Networks}\label{sec:nonlinear-function-approximation-artificial-neural-networks}

\chapter{On-policy Control with Approximation}\label{ch:on-policy-control-with-approximation}

\chapter{*Off-policy Methods with Approximation}\label{ch:off-policy-methods-with-approximation}

\chapter{Eligibility Traces}\label{ch:eligibility-traces}

\chapter{Policy Gradient Methods}\label{ch:policy-gradient-methods}

\part{Looking Deeper}\label{pt:looking-deeper}

\chapter{Psychology}\label{ch:psychology}

\chapter{Neuroscience}\label{ch:neuroscience}

\chapter{Applications and Case Studies}\label{ch:applications-and-case-studies}

\chapter{Frontiers}\label{ch:frontiers}
\section{General Value Functions and Auxiliary Tasks}
\section{Temporal Abstraction via Options}
\section{Observations and State}\label{sec:observations-and-state}

\nocite{*}
\printbibliography[title=참고~문헌]

\end{document}

